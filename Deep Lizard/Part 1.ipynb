{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental drugs scenario\n",
    "for i in range(50):\n",
    "    # The 5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13, 64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "    \n",
    "    # The 5% of older individuals who didn't experience side effects\n",
    "    random_older = randint(65, 100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "\n",
    "for i in range(1000):\n",
    "    # The majority of younger individuals\n",
    "    random_younger = randint(13, 64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "    \n",
    "    # The majority of older individuals\n",
    "    random_older = randint(65, 100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 98,\n",
       " 22,\n",
       " 83,\n",
       " 55,\n",
       " 96,\n",
       " 32,\n",
       " 65,\n",
       " 64,\n",
       " 99,\n",
       " 46,\n",
       " 98,\n",
       " 60,\n",
       " 94,\n",
       " 64,\n",
       " 98,\n",
       " 30,\n",
       " 72,\n",
       " 52,\n",
       " 92,\n",
       " 48,\n",
       " 79,\n",
       " 52,\n",
       " 91,\n",
       " 57,\n",
       " 83,\n",
       " 26,\n",
       " 88,\n",
       " 37,\n",
       " 72,\n",
       " 34,\n",
       " 67,\n",
       " 49,\n",
       " 98,\n",
       " 60,\n",
       " 72,\n",
       " 13,\n",
       " 74,\n",
       " 15,\n",
       " 84,\n",
       " 63,\n",
       " 97,\n",
       " 32,\n",
       " 88,\n",
       " 57,\n",
       " 76,\n",
       " 27,\n",
       " 86,\n",
       " 22,\n",
       " 88,\n",
       " 48,\n",
       " 85,\n",
       " 48,\n",
       " 85,\n",
       " 58,\n",
       " 70,\n",
       " 15,\n",
       " 99,\n",
       " 53,\n",
       " 74,\n",
       " 35,\n",
       " 77,\n",
       " 47,\n",
       " 97,\n",
       " 48,\n",
       " 90,\n",
       " 22,\n",
       " 72,\n",
       " 30,\n",
       " 99,\n",
       " 59,\n",
       " 73,\n",
       " 15,\n",
       " 95,\n",
       " 49,\n",
       " 88,\n",
       " 46,\n",
       " 96,\n",
       " 27,\n",
       " 67,\n",
       " 56,\n",
       " 100,\n",
       " 31,\n",
       " 94,\n",
       " 22,\n",
       " 86,\n",
       " 25,\n",
       " 82,\n",
       " 48,\n",
       " 94,\n",
       " 27,\n",
       " 91,\n",
       " 36,\n",
       " 73,\n",
       " 27,\n",
       " 66,\n",
       " 44,\n",
       " 83,\n",
       " 32,\n",
       " 74,\n",
       " 41,\n",
       " 70,\n",
       " 15,\n",
       " 83,\n",
       " 20,\n",
       " 67,\n",
       " 51,\n",
       " 90,\n",
       " 19,\n",
       " 72,\n",
       " 64,\n",
       " 70,\n",
       " 53,\n",
       " 90,\n",
       " 44,\n",
       " 87,\n",
       " 22,\n",
       " 88,\n",
       " 31,\n",
       " 85,\n",
       " 20,\n",
       " 69,\n",
       " 40,\n",
       " 95,\n",
       " 15,\n",
       " 84,\n",
       " 15,\n",
       " 86,\n",
       " 28,\n",
       " 84,\n",
       " 19,\n",
       " 82,\n",
       " 17,\n",
       " 86,\n",
       " 25,\n",
       " 82,\n",
       " 64,\n",
       " 95,\n",
       " 35,\n",
       " 72,\n",
       " 44,\n",
       " 70,\n",
       " 60,\n",
       " 87,\n",
       " 13,\n",
       " 90,\n",
       " 13,\n",
       " 90,\n",
       " 52,\n",
       " 69,\n",
       " 46,\n",
       " 94,\n",
       " 63,\n",
       " 77,\n",
       " 19,\n",
       " 70,\n",
       " 23,\n",
       " 84,\n",
       " 42,\n",
       " 96,\n",
       " 52,\n",
       " 82,\n",
       " 52,\n",
       " 66,\n",
       " 43,\n",
       " 86,\n",
       " 16,\n",
       " 93,\n",
       " 48,\n",
       " 94,\n",
       " 38,\n",
       " 97,\n",
       " 62,\n",
       " 89,\n",
       " 55,\n",
       " 89,\n",
       " 64,\n",
       " 99,\n",
       " 28,\n",
       " 66,\n",
       " 48,\n",
       " 69,\n",
       " 31,\n",
       " 92,\n",
       " 50,\n",
       " 75,\n",
       " 56,\n",
       " 92,\n",
       " 20,\n",
       " 84,\n",
       " 31,\n",
       " 85,\n",
       " 17,\n",
       " 66,\n",
       " 63,\n",
       " 85,\n",
       " 33,\n",
       " 88,\n",
       " 40,\n",
       " 70,\n",
       " 27,\n",
       " 73,\n",
       " 59,\n",
       " 72,\n",
       " 30,\n",
       " 87,\n",
       " 31,\n",
       " 100,\n",
       " 17,\n",
       " 98,\n",
       " 33,\n",
       " 73,\n",
       " 36,\n",
       " 99,\n",
       " 21,\n",
       " 86,\n",
       " 25,\n",
       " 95,\n",
       " 24,\n",
       " 83,\n",
       " 24,\n",
       " 72,\n",
       " 54,\n",
       " 97,\n",
       " 25,\n",
       " 65,\n",
       " 19,\n",
       " 80,\n",
       " 24,\n",
       " 85,\n",
       " 64,\n",
       " 88,\n",
       " 64,\n",
       " 95,\n",
       " 26,\n",
       " 86,\n",
       " 53,\n",
       " 93,\n",
       " 56,\n",
       " 85,\n",
       " 47,\n",
       " 82,\n",
       " 38,\n",
       " 71,\n",
       " 20,\n",
       " 74,\n",
       " 50,\n",
       " 100,\n",
       " 19,\n",
       " 71,\n",
       " 44,\n",
       " 85,\n",
       " 30,\n",
       " 79,\n",
       " 59,\n",
       " 90,\n",
       " 58,\n",
       " 94,\n",
       " 42,\n",
       " 85,\n",
       " 46,\n",
       " 74,\n",
       " 59,\n",
       " 100,\n",
       " 59,\n",
       " 94,\n",
       " 36,\n",
       " 96,\n",
       " 31,\n",
       " 95,\n",
       " 14,\n",
       " 98,\n",
       " 29,\n",
       " 76,\n",
       " 62,\n",
       " 91,\n",
       " 19,\n",
       " 96,\n",
       " 21,\n",
       " 75,\n",
       " 43,\n",
       " 81,\n",
       " 20,\n",
       " 67,\n",
       " 19,\n",
       " 73,\n",
       " 53,\n",
       " 79,\n",
       " 59,\n",
       " 90,\n",
       " 53,\n",
       " 84,\n",
       " 46,\n",
       " 90,\n",
       " 33,\n",
       " 98,\n",
       " 24,\n",
       " 80,\n",
       " 22,\n",
       " 83,\n",
       " 63,\n",
       " 86,\n",
       " 63,\n",
       " 78,\n",
       " 44,\n",
       " 87,\n",
       " 53,\n",
       " 94,\n",
       " 54,\n",
       " 99,\n",
       " 41,\n",
       " 70,\n",
       " 24,\n",
       " 65,\n",
       " 36,\n",
       " 83,\n",
       " 47,\n",
       " 95,\n",
       " 46,\n",
       " 67,\n",
       " 32,\n",
       " 79,\n",
       " 34,\n",
       " 79,\n",
       " 27,\n",
       " 97,\n",
       " 42,\n",
       " 69,\n",
       " 44,\n",
       " 96,\n",
       " 25,\n",
       " 82,\n",
       " 35,\n",
       " 83,\n",
       " 54,\n",
       " 93,\n",
       " 18,\n",
       " 94,\n",
       " 61,\n",
       " 69,\n",
       " 31,\n",
       " 91,\n",
       " 48,\n",
       " 81,\n",
       " 13,\n",
       " 91,\n",
       " 46,\n",
       " 71,\n",
       " 62,\n",
       " 89,\n",
       " 39,\n",
       " 72,\n",
       " 47,\n",
       " 67,\n",
       " 62,\n",
       " 92,\n",
       " 35,\n",
       " 92,\n",
       " 57,\n",
       " 92,\n",
       " 54,\n",
       " 100,\n",
       " 21,\n",
       " 89,\n",
       " 56,\n",
       " 65,\n",
       " 28,\n",
       " 98,\n",
       " 19,\n",
       " 68,\n",
       " 54,\n",
       " 72,\n",
       " 53,\n",
       " 76,\n",
       " 28,\n",
       " 86,\n",
       " 59,\n",
       " 96,\n",
       " 13,\n",
       " 71,\n",
       " 58,\n",
       " 92,\n",
       " 31,\n",
       " 89,\n",
       " 43,\n",
       " 75,\n",
       " 21,\n",
       " 92,\n",
       " 17,\n",
       " 99,\n",
       " 34,\n",
       " 75,\n",
       " 30,\n",
       " 90,\n",
       " 59,\n",
       " 90,\n",
       " 23,\n",
       " 99,\n",
       " 50,\n",
       " 92,\n",
       " 32,\n",
       " 79,\n",
       " 30,\n",
       " 100,\n",
       " 16,\n",
       " 90,\n",
       " 58,\n",
       " 70,\n",
       " 37,\n",
       " 92,\n",
       " 24,\n",
       " 99,\n",
       " 44,\n",
       " 67,\n",
       " 40,\n",
       " 81,\n",
       " 30,\n",
       " 93,\n",
       " 57,\n",
       " 65,\n",
       " 28,\n",
       " 88,\n",
       " 32,\n",
       " 65,\n",
       " 51,\n",
       " 74,\n",
       " 32,\n",
       " 79,\n",
       " 33,\n",
       " 94,\n",
       " 31,\n",
       " 68,\n",
       " 35,\n",
       " 93,\n",
       " 25,\n",
       " 77,\n",
       " 54,\n",
       " 85,\n",
       " 62,\n",
       " 88,\n",
       " 29,\n",
       " 89,\n",
       " 14,\n",
       " 84,\n",
       " 43,\n",
       " 98,\n",
       " 64,\n",
       " 82,\n",
       " 35,\n",
       " 67,\n",
       " 55,\n",
       " 89,\n",
       " 45,\n",
       " 98,\n",
       " 31,\n",
       " 70,\n",
       " 17,\n",
       " 67,\n",
       " 36,\n",
       " 83,\n",
       " 35,\n",
       " 84,\n",
       " 32,\n",
       " 99,\n",
       " 43,\n",
       " 100,\n",
       " 28,\n",
       " 98,\n",
       " 24,\n",
       " 88,\n",
       " 30,\n",
       " 71,\n",
       " 21,\n",
       " 81,\n",
       " 20,\n",
       " 69,\n",
       " 33,\n",
       " 68,\n",
       " 55,\n",
       " 77,\n",
       " 27,\n",
       " 79,\n",
       " 37,\n",
       " 100,\n",
       " 22,\n",
       " 70,\n",
       " 18,\n",
       " 88,\n",
       " 56,\n",
       " 81,\n",
       " 27,\n",
       " 93,\n",
       " 59,\n",
       " 73,\n",
       " 51,\n",
       " 79,\n",
       " 43,\n",
       " 86,\n",
       " 64,\n",
       " 80,\n",
       " 31,\n",
       " 94,\n",
       " 33,\n",
       " 90,\n",
       " 34,\n",
       " 70,\n",
       " 28,\n",
       " 92,\n",
       " 51,\n",
       " 66,\n",
       " 22,\n",
       " 65,\n",
       " 62,\n",
       " 95,\n",
       " 57,\n",
       " 74,\n",
       " 43,\n",
       " 77,\n",
       " 17,\n",
       " 75,\n",
       " 25,\n",
       " 87,\n",
       " 14,\n",
       " 90,\n",
       " 21,\n",
       " 86,\n",
       " 13,\n",
       " 84,\n",
       " 17,\n",
       " 73,\n",
       " 36,\n",
       " 75,\n",
       " 22,\n",
       " 83,\n",
       " 20,\n",
       " 94,\n",
       " 25,\n",
       " 89,\n",
       " 60,\n",
       " 71,\n",
       " 21,\n",
       " 77,\n",
       " 48,\n",
       " 76,\n",
       " 23,\n",
       " 83,\n",
       " 40,\n",
       " 94,\n",
       " 29,\n",
       " 89,\n",
       " 45,\n",
       " 89,\n",
       " 54,\n",
       " 91,\n",
       " 34,\n",
       " 87,\n",
       " 14,\n",
       " 67,\n",
       " 63,\n",
       " 93,\n",
       " 46,\n",
       " 74,\n",
       " 44,\n",
       " 92,\n",
       " 50,\n",
       " 98,\n",
       " 24,\n",
       " 78,\n",
       " 27,\n",
       " 81,\n",
       " 52,\n",
       " 88,\n",
       " 40,\n",
       " 66,\n",
       " 55,\n",
       " 84,\n",
       " 20,\n",
       " 73,\n",
       " 55,\n",
       " 89,\n",
       " 28,\n",
       " 91,\n",
       " 41,\n",
       " 80,\n",
       " 19,\n",
       " 69,\n",
       " 62,\n",
       " 86,\n",
       " 57,\n",
       " 99,\n",
       " 56,\n",
       " 91,\n",
       " 32,\n",
       " 66,\n",
       " 30,\n",
       " 85,\n",
       " 24,\n",
       " 91,\n",
       " 35,\n",
       " 75,\n",
       " 27,\n",
       " 100,\n",
       " 34,\n",
       " 89,\n",
       " 55,\n",
       " 85,\n",
       " 33,\n",
       " 77,\n",
       " 32,\n",
       " 83,\n",
       " 31,\n",
       " 87,\n",
       " 34,\n",
       " 84,\n",
       " 26,\n",
       " 68,\n",
       " 25,\n",
       " 76,\n",
       " 42,\n",
       " 96,\n",
       " 34,\n",
       " 73,\n",
       " 59,\n",
       " 85,\n",
       " 44,\n",
       " 70,\n",
       " 41,\n",
       " 95,\n",
       " 58,\n",
       " 72,\n",
       " 13,\n",
       " 69,\n",
       " 44,\n",
       " 91,\n",
       " 20,\n",
       " 95,\n",
       " 51,\n",
       " 77,\n",
       " 20,\n",
       " 69,\n",
       " 28,\n",
       " 92,\n",
       " 39,\n",
       " 77,\n",
       " 39,\n",
       " 70,\n",
       " 24,\n",
       " 71,\n",
       " 52,\n",
       " 74,\n",
       " 55,\n",
       " 82,\n",
       " 35,\n",
       " 82,\n",
       " 63,\n",
       " 94,\n",
       " 47,\n",
       " 77,\n",
       " 41,\n",
       " 72,\n",
       " 34,\n",
       " 74,\n",
       " 47,\n",
       " 79,\n",
       " 28,\n",
       " 97,\n",
       " 22,\n",
       " 91,\n",
       " 60,\n",
       " 83,\n",
       " 49,\n",
       " 96,\n",
       " 54,\n",
       " 82,\n",
       " 52,\n",
       " 82,\n",
       " 43,\n",
       " 76,\n",
       " 43,\n",
       " 78,\n",
       " 63,\n",
       " 72,\n",
       " 28,\n",
       " 94,\n",
       " 63,\n",
       " 86,\n",
       " 39,\n",
       " 76,\n",
       " 64,\n",
       " 99,\n",
       " 26,\n",
       " 74,\n",
       " 21,\n",
       " 93,\n",
       " 38,\n",
       " 77,\n",
       " 35,\n",
       " 97,\n",
       " 30,\n",
       " 91,\n",
       " 38,\n",
       " 99,\n",
       " 14,\n",
       " 74,\n",
       " 19,\n",
       " 93,\n",
       " 59,\n",
       " 85,\n",
       " 49,\n",
       " 96,\n",
       " 44,\n",
       " 93,\n",
       " 22,\n",
       " 73,\n",
       " 56,\n",
       " 84,\n",
       " 17,\n",
       " 82,\n",
       " 64,\n",
       " 100,\n",
       " 51,\n",
       " 76,\n",
       " 14,\n",
       " 79,\n",
       " 25,\n",
       " 75,\n",
       " 29,\n",
       " 88,\n",
       " 29,\n",
       " 85,\n",
       " 13,\n",
       " 77,\n",
       " 32,\n",
       " 68,\n",
       " 18,\n",
       " 99,\n",
       " 17,\n",
       " 72,\n",
       " 53,\n",
       " 65,\n",
       " 28,\n",
       " 76,\n",
       " 42,\n",
       " 70,\n",
       " 25,\n",
       " 75,\n",
       " 50,\n",
       " 81,\n",
       " 53,\n",
       " 86,\n",
       " 56,\n",
       " 77,\n",
       " 28,\n",
       " 83,\n",
       " 32,\n",
       " 76,\n",
       " 35,\n",
       " 74,\n",
       " 20,\n",
       " 74,\n",
       " 38,\n",
       " 92,\n",
       " 40,\n",
       " 79,\n",
       " 48,\n",
       " 85,\n",
       " 62,\n",
       " 72,\n",
       " 39,\n",
       " 97,\n",
       " 15,\n",
       " 65,\n",
       " 61,\n",
       " 77,\n",
       " 16,\n",
       " 67,\n",
       " 43,\n",
       " 66,\n",
       " 62,\n",
       " 96,\n",
       " 27,\n",
       " 78,\n",
       " 21,\n",
       " 72,\n",
       " 60,\n",
       " 68,\n",
       " 50,\n",
       " 89,\n",
       " 63,\n",
       " 73,\n",
       " 26,\n",
       " 65,\n",
       " 26,\n",
       " 98,\n",
       " 43,\n",
       " 93,\n",
       " 24,\n",
       " 79,\n",
       " 34,\n",
       " 75,\n",
       " 36,\n",
       " 97,\n",
       " 58,\n",
       " 92,\n",
       " 13,\n",
       " 66,\n",
       " 18,\n",
       " 67,\n",
       " 41,\n",
       " 81,\n",
       " 15,\n",
       " 86,\n",
       " 27,\n",
       " 65,\n",
       " 47,\n",
       " 100,\n",
       " 41,\n",
       " 80,\n",
       " 14,\n",
       " 83,\n",
       " 51,\n",
       " 73,\n",
       " 51,\n",
       " 81,\n",
       " 47,\n",
       " 76,\n",
       " 50,\n",
       " 76,\n",
       " 51,\n",
       " 85,\n",
       " 44,\n",
       " 84,\n",
       " 18,\n",
       " 69,\n",
       " 60,\n",
       " 71,\n",
       " 28,\n",
       " 82,\n",
       " 32,\n",
       " 87,\n",
       " 48,\n",
       " 87,\n",
       " 34,\n",
       " 92,\n",
       " 60,\n",
       " 79,\n",
       " 40,\n",
       " 89,\n",
       " 34,\n",
       " 73,\n",
       " 58,\n",
       " 83,\n",
       " 13,\n",
       " 94,\n",
       " 40,\n",
       " 75,\n",
       " 16,\n",
       " 99,\n",
       " 14,\n",
       " 66,\n",
       " 58,\n",
       " 82,\n",
       " 20,\n",
       " 66,\n",
       " 38,\n",
       " 92,\n",
       " 41,\n",
       " 89,\n",
       " 48,\n",
       " 83,\n",
       " 58,\n",
       " 85,\n",
       " 31,\n",
       " 75,\n",
       " 24,\n",
       " 72,\n",
       " 32,\n",
       " 78,\n",
       " 57,\n",
       " 68,\n",
       " 60,\n",
       " 88,\n",
       " 21,\n",
       " 85,\n",
       " 16,\n",
       " 81,\n",
       " 55,\n",
       " 96,\n",
       " 31,\n",
       " 86,\n",
       " 46,\n",
       " 89,\n",
       " 30,\n",
       " 72,\n",
       " 43,\n",
       " 95,\n",
       " 28,\n",
       " 89,\n",
       " 48,\n",
       " 73,\n",
       " 22,\n",
       " 70,\n",
       " 28,\n",
       " 76,\n",
       " 40,\n",
       " 79,\n",
       " 52,\n",
       " 72,\n",
       " 46,\n",
       " 76,\n",
       " 50,\n",
       " 73,\n",
       " 34,\n",
       " 68,\n",
       " 27,\n",
       " 80,\n",
       " 13,\n",
       " 93,\n",
       " 30,\n",
       " 77,\n",
       " 59,\n",
       " 76,\n",
       " 21,\n",
       " 66,\n",
       " 25,\n",
       " 95,\n",
       " 55,\n",
       " 77,\n",
       " 37,\n",
       " 78,\n",
       " 18,\n",
       " 100,\n",
       " 53,\n",
       " 80,\n",
       " 62,\n",
       " 70,\n",
       " 55,\n",
       " 74,\n",
       " 48,\n",
       " 92,\n",
       " 35,\n",
       " 66,\n",
       " 58,\n",
       " 65,\n",
       " 57,\n",
       " 81,\n",
       " 22,\n",
       " 83,\n",
       " 47,\n",
       " 94,\n",
       " 14,\n",
       " 69,\n",
       " 46,\n",
       " 73,\n",
       " 23,\n",
       " 95,\n",
       " 21,\n",
       " 88,\n",
       " 36,\n",
       " 82,\n",
       " 58,\n",
       " 77,\n",
       " 13,\n",
       " 83,\n",
       " 43,\n",
       " 76,\n",
       " 45,\n",
       " 77,\n",
       " 27,\n",
       " 86,\n",
       " 37,\n",
       " 87,\n",
       " 37,\n",
       " 92,\n",
       " 16,\n",
       " 68,\n",
       " 37,\n",
       " 99,\n",
       " 31,\n",
       " 91,\n",
       " 48,\n",
       " 76,\n",
       " 14,\n",
       " 69,\n",
       " 47,\n",
       " 74,\n",
       " 19,\n",
       " 70,\n",
       " 16,\n",
       " 88,\n",
       " 63,\n",
       " 67,\n",
       " 29,\n",
       " 87,\n",
       " 60,\n",
       " 83,\n",
       " 56,\n",
       " 94,\n",
       " 49,\n",
       " 76,\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View train_samples\n",
    "train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the two lists\n",
    "train_labels, train_samples = shuffle(train_labels, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[81],\n",
       "       [34],\n",
       "       [66],\n",
       "       ...,\n",
       "       [85],\n",
       "       [92],\n",
       "       [20]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the ages from a scale of 13 to 100 to 0 to 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# Fit-transform doesn't accept 1-d data\n",
    "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7816092 ],\n",
       "       [0.24137931],\n",
       "       [0.6091954 ],\n",
       "       ...,\n",
       "       [0.82758621],\n",
       "       [0.90804598],\n",
       "       [0.08045977]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the scaled data\n",
    "scaled_train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation=\"relu\"),\n",
    "    Dense(units=32, activation=\"relu\"),\n",
    "    Dense(units=2, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of input layer\n",
    "scaled_train_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2100/2100 - 2s - loss: 0.6933 - acc: 0.5843\n",
      "Epoch 2/30\n",
      "2100/2100 - 1s - loss: 0.6627 - acc: 0.6190\n",
      "Epoch 3/30\n",
      "2100/2100 - 1s - loss: 0.6278 - acc: 0.6938\n",
      "Epoch 4/30\n",
      "2100/2100 - 1s - loss: 0.5802 - acc: 0.7900\n",
      "Epoch 5/30\n",
      "2100/2100 - 1s - loss: 0.5398 - acc: 0.8157\n",
      "Epoch 6/30\n",
      "2100/2100 - 1s - loss: 0.5048 - acc: 0.8352\n",
      "Epoch 7/30\n",
      "2100/2100 - 1s - loss: 0.4717 - acc: 0.8495\n",
      "Epoch 8/30\n",
      "2100/2100 - 1s - loss: 0.4412 - acc: 0.8705\n",
      "Epoch 9/30\n",
      "2100/2100 - 1s - loss: 0.4137 - acc: 0.8805\n",
      "Epoch 10/30\n",
      "2100/2100 - 1s - loss: 0.3897 - acc: 0.8881\n",
      "Epoch 11/30\n",
      "2100/2100 - 1s - loss: 0.3690 - acc: 0.9019\n",
      "Epoch 12/30\n",
      "2100/2100 - 1s - loss: 0.3519 - acc: 0.9019\n",
      "Epoch 13/30\n",
      "2100/2100 - 1s - loss: 0.3377 - acc: 0.9071\n",
      "Epoch 14/30\n",
      "2100/2100 - 1s - loss: 0.3258 - acc: 0.9143\n",
      "Epoch 15/30\n",
      "2100/2100 - 1s - loss: 0.3158 - acc: 0.9119\n",
      "Epoch 16/30\n",
      "2100/2100 - 1s - loss: 0.3072 - acc: 0.9181\n",
      "Epoch 17/30\n",
      "2100/2100 - 1s - loss: 0.3001 - acc: 0.9171\n",
      "Epoch 18/30\n",
      "2100/2100 - 1s - loss: 0.2943 - acc: 0.9195\n",
      "Epoch 19/30\n",
      "2100/2100 - 1s - loss: 0.2892 - acc: 0.9195\n",
      "Epoch 20/30\n",
      "2100/2100 - 1s - loss: 0.2849 - acc: 0.9205\n",
      "Epoch 21/30\n",
      "2100/2100 - 1s - loss: 0.2814 - acc: 0.9267\n",
      "Epoch 22/30\n",
      "2100/2100 - 1s - loss: 0.2781 - acc: 0.9233\n",
      "Epoch 23/30\n",
      "2100/2100 - 1s - loss: 0.2753 - acc: 0.9238\n",
      "Epoch 24/30\n",
      "2100/2100 - 1s - loss: 0.2731 - acc: 0.9314\n",
      "Epoch 25/30\n",
      "2100/2100 - 1s - loss: 0.2709 - acc: 0.9324\n",
      "Epoch 26/30\n",
      "2100/2100 - 1s - loss: 0.2690 - acc: 0.9271\n",
      "Epoch 27/30\n",
      "2100/2100 - 1s - loss: 0.2674 - acc: 0.9324\n",
      "Epoch 28/30\n",
      "2100/2100 - 1s - loss: 0.2659 - acc: 0.9305\n",
      "Epoch 29/30\n",
      "2100/2100 - 1s - loss: 0.2646 - acc: 0.9348\n",
      "Epoch 30/30\n",
      "2100/2100 - 1s - loss: 0.2635 - acc: 0.9324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24e774308c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_train_samples, y=train_labels, batch_size=10, epochs=30, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1890 samples, validate on 210 samples\n",
      "Epoch 1/30\n",
      "1890/1890 - 1s - loss: 0.2636 - acc: 0.9333 - val_loss: 0.2526 - val_acc: 0.9333\n",
      "Epoch 2/30\n",
      "1890/1890 - 1s - loss: 0.2627 - acc: 0.9323 - val_loss: 0.2512 - val_acc: 0.9333\n",
      "Epoch 3/30\n",
      "1890/1890 - 1s - loss: 0.2620 - acc: 0.9339 - val_loss: 0.2498 - val_acc: 0.9524\n",
      "Epoch 4/30\n",
      "1890/1890 - 1s - loss: 0.2614 - acc: 0.9354 - val_loss: 0.2491 - val_acc: 0.9333\n",
      "Epoch 5/30\n",
      "1890/1890 - 1s - loss: 0.2606 - acc: 0.9333 - val_loss: 0.2484 - val_acc: 0.9333\n",
      "Epoch 6/30\n",
      "1890/1890 - 1s - loss: 0.2601 - acc: 0.9328 - val_loss: 0.2478 - val_acc: 0.9333\n",
      "Epoch 7/30\n",
      "1890/1890 - 2s - loss: 0.2596 - acc: 0.9328 - val_loss: 0.2466 - val_acc: 0.9333\n",
      "Epoch 8/30\n",
      "1890/1890 - 1s - loss: 0.2592 - acc: 0.9360 - val_loss: 0.2460 - val_acc: 0.9333\n",
      "Epoch 9/30\n",
      "1890/1890 - 2s - loss: 0.2587 - acc: 0.9328 - val_loss: 0.2451 - val_acc: 0.9524\n",
      "Epoch 10/30\n",
      "1890/1890 - 2s - loss: 0.2580 - acc: 0.9423 - val_loss: 0.2452 - val_acc: 0.9333\n",
      "Epoch 11/30\n",
      "1890/1890 - 2s - loss: 0.2579 - acc: 0.9360 - val_loss: 0.2451 - val_acc: 0.9333\n",
      "Epoch 12/30\n",
      "1890/1890 - 1s - loss: 0.2574 - acc: 0.9339 - val_loss: 0.2432 - val_acc: 0.9524\n",
      "Epoch 13/30\n",
      "1890/1890 - 1s - loss: 0.2571 - acc: 0.9339 - val_loss: 0.2431 - val_acc: 0.9333\n",
      "Epoch 14/30\n",
      "1890/1890 - 1s - loss: 0.2568 - acc: 0.9402 - val_loss: 0.2421 - val_acc: 0.9524\n",
      "Epoch 15/30\n",
      "1890/1890 - 1s - loss: 0.2562 - acc: 0.9392 - val_loss: 0.2419 - val_acc: 0.9524\n",
      "Epoch 16/30\n",
      "1890/1890 - 1s - loss: 0.2560 - acc: 0.9386 - val_loss: 0.2414 - val_acc: 0.9524\n",
      "Epoch 17/30\n",
      "1890/1890 - 1s - loss: 0.2554 - acc: 0.9429 - val_loss: 0.2415 - val_acc: 0.9333\n",
      "Epoch 18/30\n",
      "1890/1890 - 1s - loss: 0.2551 - acc: 0.9381 - val_loss: 0.2410 - val_acc: 0.9333\n",
      "Epoch 19/30\n",
      "1890/1890 - 1s - loss: 0.2546 - acc: 0.9365 - val_loss: 0.2396 - val_acc: 0.9524\n",
      "Epoch 20/30\n",
      "1890/1890 - 1s - loss: 0.2544 - acc: 0.9397 - val_loss: 0.2393 - val_acc: 0.9524\n",
      "Epoch 21/30\n",
      "1890/1890 - 1s - loss: 0.2541 - acc: 0.9429 - val_loss: 0.2395 - val_acc: 0.9524\n",
      "Epoch 22/30\n",
      "1890/1890 - 1s - loss: 0.2538 - acc: 0.9418 - val_loss: 0.2394 - val_acc: 0.9524\n",
      "Epoch 23/30\n",
      "1890/1890 - 1s - loss: 0.2533 - acc: 0.9365 - val_loss: 0.2383 - val_acc: 0.9524\n",
      "Epoch 24/30\n",
      "1890/1890 - 1s - loss: 0.2530 - acc: 0.9418 - val_loss: 0.2383 - val_acc: 0.9524\n",
      "Epoch 25/30\n",
      "1890/1890 - 1s - loss: 0.2527 - acc: 0.9429 - val_loss: 0.2385 - val_acc: 0.9524\n",
      "Epoch 26/30\n",
      "1890/1890 - 1s - loss: 0.2524 - acc: 0.9392 - val_loss: 0.2375 - val_acc: 0.9524\n",
      "Epoch 27/30\n",
      "1890/1890 - 1s - loss: 0.2521 - acc: 0.9429 - val_loss: 0.2373 - val_acc: 0.9524\n",
      "Epoch 28/30\n",
      "1890/1890 - 1s - loss: 0.2519 - acc: 0.9429 - val_loss: 0.2368 - val_acc: 0.9524\n",
      "Epoch 29/30\n",
      "1890/1890 - 1s - loss: 0.2515 - acc: 0.9429 - val_loss: 0.2365 - val_acc: 0.9524\n",
      "Epoch 30/30\n",
      "1890/1890 - 1s - loss: 0.2513 - acc: 0.9429 - val_loss: 0.2362 - val_acc: 0.9524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24e786075c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get metrics for validation set\n",
    "model.fit(x=scaled_train_samples, y=train_labels, validation_split=0.1, batch_size=10, epochs=30, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental drugs scenario\n",
    "for i in range(10):\n",
    "    # The 5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13, 64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "    \n",
    "    # The 5% of older individuals who didn't experience side effects\n",
    "    random_older = randint(65, 100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "\n",
    "for i in range(200):\n",
    "    # The majority of younger individuals\n",
    "    random_younger = randint(13, 64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "    # The majority of older individuals\n",
    "    random_older = randint(65, 100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into numpy arrays\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the two lists\n",
    "test_labels, test_samples = shuffle(test_labels, test_samples)\n",
    "\n",
    "# Scale the data features\n",
    "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=scaled_test_samples, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07773572 0.9222643 ]\n",
      "[0.9747049  0.02529501]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.94359726 0.05640272]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.04181827 0.95818174]\n",
      "[0.11283044 0.8871696 ]\n",
      "[0.936816   0.06318399]\n",
      "[0.9740906  0.02590937]\n",
      "[0.9742066 0.0257934]\n",
      "[0.09038279 0.9096172 ]\n",
      "[0.16137594 0.83862406]\n",
      "[0.96922505 0.03077491]\n",
      "[0.9708439 0.0291561]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.9739742  0.02602585]\n",
      "[0.7032138  0.29678616]\n",
      "[0.975155   0.02484501]\n",
      "[0.97493094 0.02506902]\n",
      "[0.97498715 0.02501284]\n",
      "[0.08384287 0.9161572 ]\n",
      "[0.894353   0.10564696]\n",
      "[0.02209866 0.97790134]\n",
      "[0.03864308 0.9613569 ]\n",
      "[0.9675194  0.03248058]\n",
      "[0.96382993 0.03617002]\n",
      "[0.10485352 0.8951464 ]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.9494277  0.05057232]\n",
      "[0.04181827 0.95818174]\n",
      "[0.0203889  0.97961116]\n",
      "[0.96382993 0.03617002]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.97373974 0.02626033]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.5224995  0.47750047]\n",
      "[0.97504324 0.02495677]\n",
      "[0.04181827 0.95818174]\n",
      "[0.9740906  0.02590937]\n",
      "[0.05290589 0.94709414]\n",
      "[0.03044849 0.9695515 ]\n",
      "[0.05290589 0.94709414]\n",
      "[0.96922505 0.03077491]\n",
      "[0.7771171  0.22288294]\n",
      "[0.97521067 0.02478931]\n",
      "[0.97453415 0.0254658 ]\n",
      "[0.9746482  0.02535182]\n",
      "[0.9675194  0.03248058]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.11283044 0.8871696 ]\n",
      "[0.97373974 0.02626033]\n",
      "[0.9708439 0.0291561]\n",
      "[0.97493094 0.02506902]\n",
      "[0.33569655 0.6643035 ]\n",
      "[0.9731513  0.02684869]\n",
      "[0.03864308 0.9613569 ]\n",
      "[0.9750992  0.02490082]\n",
      "[0.908185   0.09181502]\n",
      "[0.01734913 0.9826508 ]\n",
      "[0.09038279 0.9096172 ]\n",
      "[0.04524204 0.9547579 ]\n",
      "[0.96147114 0.03852889]\n",
      "[0.03864308 0.9613569 ]\n",
      "[0.25562602 0.744374  ]\n",
      "[0.97373974 0.02626033]\n",
      "[0.97476166 0.02523833]\n",
      "[0.97476166 0.02523833]\n",
      "[0.7771171  0.22288294]\n",
      "[0.01880888 0.9811911 ]\n",
      "[0.9748746  0.02512534]\n",
      "[0.97245514 0.02754483]\n",
      "[0.9739742  0.02602585]\n",
      "[0.10485352 0.8951464 ]\n",
      "[0.908185   0.09181502]\n",
      "[0.9736216  0.02637834]\n",
      "[0.04181827 0.95818174]\n",
      "[0.0329734  0.96702665]\n",
      "[0.01600082 0.9839992 ]\n",
      "[0.9494277  0.05057232]\n",
      "[0.66139156 0.33860835]\n",
      "[0.03570004 0.9642999 ]\n",
      "[0.0203889  0.97961116]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.9675194  0.03248058]\n",
      "[0.936816   0.06318399]\n",
      "[0.97443706 0.02556298]\n",
      "[0.97443706 0.02556298]\n",
      "[0.9292805  0.07071944]\n",
      "[0.02209866 0.97790134]\n",
      "[0.0203889  0.97961116]\n",
      "[0.0203889  0.97961116]\n",
      "[0.10485352 0.8951464 ]\n",
      "[0.01880888 0.9811911 ]\n",
      "[0.0203889  0.97961116]\n",
      "[0.908185   0.09181502]\n",
      "[0.18921436 0.81078565]\n",
      "[0.9750992  0.02490082]\n",
      "[0.03864308 0.9613569 ]\n",
      "[0.10485352 0.8951464 ]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.04524204 0.9547579 ]\n",
      "[0.570332   0.42966804]\n",
      "[0.97385716 0.02614283]\n",
      "[0.29407382 0.70592624]\n",
      "[0.96382993 0.03617002]\n",
      "[0.9675194  0.03248058]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.97476166 0.02523833]\n",
      "[0.9748746  0.02512534]\n",
      "[0.94359726 0.05640272]\n",
      "[0.29407382 0.70592624]\n",
      "[0.96147114 0.03852889]\n",
      "[0.09038279 0.9096172 ]\n",
      "[0.7771171  0.22288294]\n",
      "[0.7771171  0.22288294]\n",
      "[0.0329734  0.96702665]\n",
      "[0.9752662  0.02473373]\n",
      "[0.9750992  0.02490082]\n",
      "[0.975155   0.02484501]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.97385716 0.02614283]\n",
      "[0.12154609 0.87845397]\n",
      "[0.894353   0.10564696]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.04524204 0.9547579 ]\n",
      "[0.04181827 0.95818174]\n",
      "[0.04181827 0.95818174]\n",
      "[0.9750992  0.02490082]\n",
      "[0.3800386  0.61996144]\n",
      "[0.3800386  0.61996144]\n",
      "[0.12154609 0.87845397]\n",
      "[0.33569655 0.6643035 ]\n",
      "[0.96922505 0.03077491]\n",
      "[0.04524204 0.9547579 ]\n",
      "[0.12154609 0.87845397]\n",
      "[0.97459126 0.02540874]\n",
      "[0.9717415  0.02825852]\n",
      "[0.08384287 0.9161572 ]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.975155   0.02484501]\n",
      "[0.13736773 0.8626323 ]\n",
      "[0.09737864 0.9026213 ]\n",
      "[0.06178394 0.9382161 ]\n",
      "[0.9747049  0.02529501]\n",
      "[0.9494277  0.05057232]\n",
      "[0.11283044 0.8871696 ]\n",
      "[0.05718328 0.94281673]\n",
      "[0.47425044 0.5257495 ]\n",
      "[0.02811126 0.9718888 ]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.908185   0.09181502]\n",
      "[0.97453415 0.0254658 ]\n",
      "[0.9731513  0.02684869]\n",
      "[0.07773572 0.9222643 ]\n",
      "[0.97385716 0.02614283]\n",
      "[0.0239483 0.9760517]\n",
      "[0.9731513  0.02684869]\n",
      "[0.97385716 0.02614283]\n",
      "[0.03570004 0.9642999 ]\n",
      "[0.03570004 0.9642999 ]\n",
      "[0.9731513  0.02684869]\n",
      "[0.02811126 0.9718888 ]\n",
      "[0.9742066 0.0257934]\n",
      "[0.80877846 0.19122158]\n",
      "[0.975155   0.02484501]\n",
      "[0.18921436 0.81078565]\n",
      "[0.09038279 0.9096172 ]\n",
      "[0.0203889  0.97961116]\n",
      "[0.3800386  0.61996144]\n",
      "[0.9736216  0.02637834]\n",
      "[0.10485352 0.8951464 ]\n",
      "[0.47425044 0.5257495 ]\n",
      "[0.10485352 0.8951464 ]\n",
      "[0.04524204 0.9547579 ]\n",
      "[0.80877846 0.19122158]\n",
      "[0.04181827 0.95818174]\n",
      "[0.9736216  0.02637834]\n",
      "[0.0667285  0.93327147]\n",
      "[0.97504324 0.02495677]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.9675194  0.03248058]\n",
      "[0.97459126 0.02540874]\n",
      "[0.83688617 0.16311382]\n",
      "[0.96382993 0.03617002]\n",
      "[0.3800386  0.61996144]\n",
      "[0.97373974 0.02626033]\n",
      "[0.04181827 0.95818174]\n",
      "[0.05290589 0.94709414]\n",
      "[0.9748746  0.02512534]\n",
      "[0.18921436 0.81078565]\n",
      "[0.936816   0.06318399]\n",
      "[0.04524204 0.9547579 ]\n",
      "[0.07773572 0.9222643 ]\n",
      "[0.97521067 0.02478931]\n",
      "[0.908185   0.09181502]\n",
      "[0.9742066 0.0257934]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.7032138  0.29678616]\n",
      "[0.96922505 0.03077491]\n",
      "[0.10485352 0.8951464 ]\n",
      "[0.9746482  0.02535182]\n",
      "[0.9740906  0.02590937]\n",
      "[0.18921436 0.81078565]\n",
      "[0.13736773 0.8626323 ]\n",
      "[0.13736773 0.8626323 ]\n",
      "[0.03044849 0.9695515 ]\n",
      "[0.7032138  0.29678616]\n",
      "[0.9740906  0.02590937]\n",
      "[0.97373974 0.02626033]\n",
      "[0.12154609 0.87845397]\n",
      "[0.0239483 0.9760517]\n",
      "[0.97459126 0.02540874]\n",
      "[0.02811126 0.9718888 ]\n",
      "[0.97373974 0.02626033]\n",
      "[0.0239483 0.9760517]\n",
      "[0.87828004 0.12171995]\n",
      "[0.80877846 0.19122158]\n",
      "[0.894353   0.10564696]\n",
      "[0.13736773 0.8626323 ]\n",
      "[0.01880888 0.9811911 ]\n",
      "[0.9740906  0.02590937]\n",
      "[0.25562602 0.744374  ]\n",
      "[0.05290589 0.94709414]\n",
      "[0.25562602 0.744374  ]\n",
      "[0.9742066 0.0257934]\n",
      "[0.3800386  0.61996144]\n",
      "[0.97443706 0.02556298]\n",
      "[0.9731513  0.02684869]\n",
      "[0.03044849 0.9695515 ]\n",
      "[0.9717415  0.02825852]\n",
      "[0.9743221  0.02567794]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.03864308 0.9613569 ]\n",
      "[0.94359726 0.05640272]\n",
      "[0.9675194  0.03248058]\n",
      "[0.97459126 0.02540874]\n",
      "[0.97245514 0.02754483]\n",
      "[0.9743221  0.02567794]\n",
      "[0.0203889  0.97961116]\n",
      "[0.95411354 0.04588649]\n",
      "[0.9581719  0.04182813]\n",
      "[0.02811126 0.9718888 ]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.96922505 0.03077491]\n",
      "[0.03044849 0.9695515 ]\n",
      "[0.9675194  0.03248058]\n",
      "[0.936816   0.06318399]\n",
      "[0.09038279 0.9096172 ]\n",
      "[0.29407382 0.70592624]\n",
      "[0.9675194  0.03248058]\n",
      "[0.9494277  0.05057232]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.05290589 0.94709414]\n",
      "[0.03044849 0.9695515 ]\n",
      "[0.3800386  0.61996144]\n",
      "[0.97385716 0.02614283]\n",
      "[0.97373974 0.02626033]\n",
      "[0.9743221  0.02567794]\n",
      "[0.9747049  0.02529501]\n",
      "[0.09737864 0.9026213 ]\n",
      "[0.9201306  0.07986944]\n",
      "[0.07773572 0.9222643 ]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.11283044 0.8871696 ]\n",
      "[0.9752662  0.02473373]\n",
      "[0.05290589 0.94709414]\n",
      "[0.47425044 0.5257495 ]\n",
      "[0.02594864 0.97405136]\n",
      "[0.33569655 0.6643035 ]\n",
      "[0.08384287 0.9161572 ]\n",
      "[0.29407382 0.70592624]\n",
      "[0.05718328 0.94281673]\n",
      "[0.01600082 0.9839992 ]\n",
      "[0.95411354 0.04588649]\n",
      "[0.9742066 0.0257934]\n",
      "[0.66139156 0.33860835]\n",
      "[0.7032138  0.29678616]\n",
      "[0.04524204 0.9547579 ]\n",
      "[0.86014414 0.13985585]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.06178394 0.9382161 ]\n",
      "[0.96382993 0.03617002]\n",
      "[0.09038279 0.9096172 ]\n",
      "[0.97521067 0.02478931]\n",
      "[0.09737864 0.9026213 ]\n",
      "[0.87828004 0.12171995]\n",
      "[0.9740906  0.02590937]\n",
      "[0.66139156 0.33860835]\n",
      "[0.94359726 0.05640272]\n",
      "[0.97443706 0.02556298]\n",
      "[0.07773572 0.9222643 ]\n",
      "[0.80877846 0.19122158]\n",
      "[0.936816   0.06318399]\n",
      "[0.9743221  0.02567794]\n",
      "[0.97443706 0.02556298]\n",
      "[0.0203889  0.97961116]\n",
      "[0.95411354 0.04588649]\n",
      "[0.08384287 0.9161572 ]\n",
      "[0.13736773 0.8626323 ]\n",
      "[0.04181827 0.95818174]\n",
      "[0.16137594 0.83862406]\n",
      "[0.0203889  0.97961116]\n",
      "[0.96572256 0.03427747]\n",
      "[0.42647776 0.5735222 ]\n",
      "[0.97453415 0.0254658 ]\n",
      "[0.9675194  0.03248058]\n",
      "[0.7032138  0.29678616]\n",
      "[0.05718328 0.94281673]\n",
      "[0.97385716 0.02614283]\n",
      "[0.9717415  0.02825852]\n",
      "[0.9581719  0.04182813]\n",
      "[0.01600082 0.9839992 ]\n",
      "[0.06178394 0.9382161 ]\n",
      "[0.9748182  0.02518177]\n",
      "[0.0667285  0.93327147]\n",
      "[0.9740906  0.02590937]\n",
      "[0.3800386  0.61996144]\n",
      "[0.06178394 0.9382161 ]\n",
      "[0.04181827 0.95818174]\n",
      "[0.03044849 0.9695515 ]\n",
      "[0.25562602 0.744374  ]\n",
      "[0.0239483 0.9760517]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.09038279 0.9096172 ]\n",
      "[0.97476166 0.02523833]\n",
      "[0.02209866 0.97790134]\n",
      "[0.9739742  0.02602585]\n",
      "[0.04524204 0.9547579 ]\n",
      "[0.07203846 0.9279615 ]\n",
      "[0.97373974 0.02626033]\n",
      "[0.12154609 0.87845397]\n",
      "[0.7032138  0.29678616]\n",
      "[0.83688617 0.16311382]\n",
      "[0.908185   0.09181502]\n",
      "[0.9708439 0.0291561]\n",
      "[0.02594864 0.97405136]\n",
      "[0.0667285  0.93327147]\n",
      "[0.66139156 0.33860835]\n",
      "[0.05718328 0.94281673]\n",
      "[0.03044849 0.9695515 ]\n",
      "[0.936816   0.06318399]\n",
      "[0.9742066 0.0257934]\n",
      "[0.96382993 0.03617002]\n",
      "[0.0667285  0.93327147]\n",
      "[0.03044849 0.9695515 ]\n",
      "[0.97476166 0.02523833]\n",
      "[0.66139156 0.33860835]\n",
      "[0.16137594 0.83862406]\n",
      "[0.09737864 0.9026213 ]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.03864308 0.9613569 ]\n",
      "[0.97453415 0.0254658 ]\n",
      "[0.42647776 0.5735222 ]\n",
      "[0.97385716 0.02614283]\n",
      "[0.97493094 0.02506902]\n",
      "[0.47425044 0.5257495 ]\n",
      "[0.9752662  0.02473373]\n",
      "[0.96382993 0.03617002]\n",
      "[0.97385716 0.02614283]\n",
      "[0.0329734  0.96702665]\n",
      "[0.9675194  0.03248058]\n",
      "[0.97498715 0.02501284]\n",
      "[0.07773572 0.9222643 ]\n",
      "[0.9717415  0.02825852]\n",
      "[0.97498715 0.02501284]\n",
      "[0.01880888 0.9811911 ]\n",
      "[0.09038279 0.9096172 ]\n",
      "[0.83688617 0.16311382]\n",
      "[0.29407382 0.70592624]\n",
      "[0.96922505 0.03077491]\n",
      "[0.9746482  0.02535182]\n",
      "[0.9752662  0.02473373]\n",
      "[0.87828004 0.12171995]\n",
      "[0.0329734  0.96702665]\n",
      "[0.0329734  0.96702665]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.9708439 0.0291561]\n",
      "[0.97245514 0.02754483]\n",
      "[0.03864308 0.9613569 ]\n",
      "[0.04181827 0.95818174]\n",
      "[0.02594864 0.97405136]\n",
      "[0.9708439 0.0291561]\n",
      "[0.12154609 0.87845397]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.03570004 0.9642999 ]\n",
      "[0.05718328 0.94281673]\n",
      "[0.97504324 0.02495677]\n",
      "[0.9675194  0.03248058]\n",
      "[0.96572256 0.03427747]\n",
      "[0.0203889  0.97961116]\n",
      "[0.0667285  0.93327147]\n",
      "[0.04893184 0.9510682 ]\n",
      "[0.05718328 0.94281673]\n",
      "[0.01734913 0.9826508 ]\n",
      "[0.13736773 0.8626323 ]\n",
      "[0.06178394 0.9382161 ]\n",
      "[0.02594864 0.97405136]\n",
      "[0.16137594 0.83862406]\n",
      "[0.42647776 0.5735222 ]\n",
      "[0.0329734  0.96702665]\n",
      "[0.03570004 0.9642999 ]\n",
      "[0.66139156 0.33860835]\n",
      "[0.9743221  0.02567794]\n",
      "[0.01600082 0.9839992 ]\n",
      "[0.22063407 0.7793659 ]\n",
      "[0.03570004 0.9642999 ]\n",
      "[0.570332   0.42966804]\n",
      "[0.97498715 0.02501284]\n",
      "[0.9201306  0.07986944]\n",
      "[0.16137594 0.83862406]\n",
      "[0.08384287 0.9161572 ]\n",
      "[0.9743221  0.02567794]\n",
      "[0.74188656 0.25811344]\n",
      "[0.9292805  0.07071944]\n",
      "[0.9748746  0.02512534]\n",
      "[0.9292805  0.07071944]\n",
      "[0.6168865 0.3831135]\n",
      "[0.66139156 0.33860835]\n"
     ]
    }
   ],
   "source": [
    "for i in predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in rounded_predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
